{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/data61/python-paillier.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgkWIKXTWZ2t",
    "outputId": "0a0fe365-3ce6-4e7d-9f28-dc2d657cf9a6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "np.random.seed(99)\n",
    "\n",
    "def load_data_from_jsonl(file_path):\n",
    "    #Reads JSONL file and extracts email texts and their corresponding labels.\n",
    "    texts, labels = [], []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            texts.append(entry['text'])  # Adapt based on the JSON schema\n",
    "            labels.append(1 if entry['label_text'] == 'spam' else -1)  # Convert labels to numerical\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "def process_datasets(train_file, test_file):\n",
    "    #Load, preprocess, and transform training and testing datasets.\n",
    "    print(\"Reading training dataset.\")\n",
    "    train_texts, train_labels = load_data_from_jsonl(train_file)\n",
    "\n",
    "    print(\"Reading testing dataset.\")\n",
    "    test_texts, test_labels = load_data_from_jsonl(test_file)\n",
    "\n",
    "    # Text vectorization (Bag-of-Words with Tfidf, without IDF)\n",
    "    vectorizer = TfidfVectorizer(use_idf=False, norm=None, stop_words='english', min_df=0.001)\n",
    "\n",
    "    # Fit vectorizer on training data and apply to test data\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "    print(f'Feature space size: {X_train.shape[1]}')\n",
    "\n",
    "    # Ensure both classes are present in training set\n",
    "    if len(np.unique(train_labels)) < 2:\n",
    "        raise ValueError(\"Training data must have both spam and non-spam labels.\")\n",
    "\n",
    "    print(f\"Proportion of spam: {np.mean(train_labels == 1):.2f}, ham: {np.mean(train_labels == -1):.2f}\")\n",
    "\n",
    "    return X_train, train_labels, X_test, test_labels\n",
    "\n",
    "@contextmanager\n",
    "def measure_time():\n",
    "    #Context manager to measure execution time.\n",
    "    start = time.perf_counter()\n",
    "    yield\n",
    "    end = time.perf_counter()\n",
    "    elapsed = end - start\n",
    "    print(f'Elapsed time: {elapsed:.2f} seconds')\n",
    "\n",
    "class SystemAdmin:\n",
    "    # Responsible for model training\n",
    "    def __init__(self):\n",
    "        self.svm_model = SVC(kernel='linear')\n",
    "        self.lr_model = LogisticRegression()\n",
    "        self.perceptron_model = Perceptron()\n",
    "\n",
    "    def train_models(self, X, y):\n",
    "        #Train SVM, Logistic Regression, and Perceptron models.\n",
    "        self.svm_model.fit(X, y)\n",
    "        self.lr_model.fit(X, y)\n",
    "        self.perceptron_model.fit(X, y)\n",
    "\n",
    "    def make_prediction(self, model, X):\n",
    "        #Make predictions with the specified model.\n",
    "        return model.predict(X)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_path = 'train.jsonl'\n",
    "    test_path = 'test.jsonl'\n",
    "\n",
    "    X_train, y_train, X_test, y_test = process_datasets(train_path, test_path)\n",
    "\n",
    "    # Admin operations\n",
    "    admin = SystemAdmin()\n",
    "\n",
    "    # Train models and record training time\n",
    "    training_times = {}\n",
    "    print(\"Training models.\")\n",
    "    for model_name, model in zip([\"SVM\", \"Logistic Regression\", \"Perceptron\"],\n",
    "                                 [admin.svm_model, admin.lr_model, admin.perceptron_model]):\n",
    "        start_time = time.perf_counter()\n",
    "        admin.train_models(X_train, y_train)\n",
    "        training_times[model_name] = time.perf_counter() - start_time\n",
    "\n",
    "    # Evaluate models and store results\n",
    "    results = []\n",
    "    for model_name, model in zip([\"SVM\", \"Logistic Regression\", \"Perceptron\"],\n",
    "                                 [admin.svm_model, admin.lr_model, admin.perceptron_model]):\n",
    "        # Unencrypted evaluation\n",
    "        start_time = time.perf_counter()\n",
    "        preds = admin.make_prediction(model, X_test)\n",
    "        test_time = time.perf_counter() - start_time\n",
    "\n",
    "        error_rate = np.mean(preds != y_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        results.append((model_name, \"Unencrypted\", error_rate, training_times[model_name], test_time, f1))\n",
    "\n",
    "    # Display results in a DataFrame\n",
    "    df_results = pd.DataFrame(results, columns=[\"Model\", \"Type\", \"Test Error\", \"Training Time\", \"Test Time\", \"F1 Score\"])\n",
    "    print(\"\\nResults Comparison:\")\n",
    "    print(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for unencrypted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using train dataset for now\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Training\n",
    "# svm_model = SVC(kernel='linear', random_state=42)\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# y_pred = svm_model.predict(X_test)\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR for unencrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Logistic Regression Model\n",
    "# log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "# log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Detailed classification report\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paillier Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os.path\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import phe as paillier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_data_from_jsonl(file_path):\n",
    "    \"\"\"Load data from a JSONL file.\"\"\"\n",
    "    emails = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            emails.append(entry['text'])  # Adjust based on your JSON field\n",
    "            labels.append(1 if entry['label_text'] == 'spam' else -1)  # Adjust label logic as necessary\n",
    "    return np.array(emails), np.array(labels)\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Get the Enron e-mails from disk.\n",
    "    Represent them as bag-of-words.\n",
    "    Shuffle and split train/test.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Importing dataset from disk...\")\n",
    "    # Load emails from your JSONL file\n",
    "    emails, labels = load_data_from_jsonl(file_path)\n",
    "\n",
    "    # Words count, keep only frequent words\n",
    "    count_vect = CountVectorizer(decode_error='replace', stop_words='english',\n",
    "                                  min_df=0.001)\n",
    "    X = count_vect.fit_transform(emails)\n",
    "\n",
    "    print('Vocabulary size: %d' % X.shape[1])\n",
    "\n",
    "    # Shuffle\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    X, y = X[perm, :], labels[perm]\n",
    "\n",
    "    # Split train and test\n",
    "    split = 500  # Number of samples to use for training\n",
    "    X_train, X_test = X[-split:, :], X[:-split, :]\n",
    "    y_train, y_test = y[-split:], y[:-split]\n",
    "\n",
    "    # Check unique labels in training set\n",
    "    unique_labels = np.unique(y_train)\n",
    "    print(\"Unique labels in training set:\", unique_labels)\n",
    "    print(y_train)\n",
    "\n",
    "    # Ensure both classes are present\n",
    "    if len(unique_labels) < 2:\n",
    "        raise ValueError(\"Training set must contain both spam and ham examples.\")\n",
    "\n",
    "    print(\"Labels in trainset are {:.2f} spam : {:.2f} ham\".format(\n",
    "        np.mean(y_train == 1), np.mean(y_train == -1)))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer():\n",
    "    \"\"\"Helper for measuring runtime\"\"\"\n",
    "    time0 = time.perf_counter()\n",
    "    yield\n",
    "    print('[elapsed time: %.2f s]' % (time.perf_counter() - time0))\n",
    "\n",
    "\n",
    "class Alice:\n",
    "    \"\"\"Represents the model owner who trains the SVM model.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = SVC(kernel='linear')  # SVM with linear kernel\n",
    "\n",
    "    def generate_paillier_keypair(self, n_length):\n",
    "        self.pubkey, self.privkey = paillier.generate_paillier_keypair(n_length=n_length)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def encrypt_weights(self):\n",
    "        # Convert sparse matrix to dense format (NumPy array)\n",
    "        coef = self.model.coef_.toarray()[0, :]\n",
    "        # Encrypt the coefficients\n",
    "        encrypted_weights = [self.pubkey.encrypt(float(coef[i])) for i in range(coef.shape[0])]\n",
    "        # Encrypt the intercept\n",
    "        encrypted_intercept = self.pubkey.encrypt(float(self.model.intercept_[0]))\n",
    "        return encrypted_weights, encrypted_intercept\n",
    "\n",
    "    def decrypt_scores(self, encrypted_scores):\n",
    "        return [self.privkey.decrypt(s) for s in encrypted_scores]\n",
    "\n",
    "\n",
    "\n",
    "class Bob:\n",
    "    \"\"\"Scores local plaintext data using the encrypted model.\"\"\"\n",
    "    def __init__(self, pubkey):\n",
    "        self.pubkey = pubkey\n",
    "\n",
    "    def set_weights(self, weights, intercept):\n",
    "        self.weights = weights\n",
    "        self.intercept = intercept\n",
    "\n",
    "    def encrypted_score(self, x):\n",
    "        score = self.intercept\n",
    "        _, idx = x.nonzero()\n",
    "        for i in idx:\n",
    "            score += x[0, i] * self.weights[i]\n",
    "        return score\n",
    "\n",
    "    def encrypted_evaluate(self, X):\n",
    "        return [self.encrypted_score(X[i, :]) for i in range(X.shape[0])]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = 'test.jsonl'  # Specify your JSONL file path here\n",
    "    X, y, X_test, y_test = preprocess_data(file_path)\n",
    "\n",
    "    print(\"Alice: Generating paillier keypair\")\n",
    "    alice = Alice()\n",
    "    alice.generate_paillier_keypair(n_length=1024)\n",
    "\n",
    "    print(\"Alice: Learning spam classifier\")\n",
    "    with timer() as t:\n",
    "        alice.fit(X, y)\n",
    "\n",
    "    print(\"Classify with model in the clear -- what Alice would get having Bob's data locally\")\n",
    "    with timer() as t:\n",
    "        error = np.mean(alice.predict(X_test) != y_test)\n",
    "    print(\"Error {:.3f}\".format(error))\n",
    "\n",
    "    print(\"Alice: Encrypting classifier\")\n",
    "    with timer() as t:\n",
    "        encrypted_weights, encrypted_intercept = alice.encrypt_weights()\n",
    "\n",
    "    print(\"Bob: Scoring with encrypted classifier\")\n",
    "    bob = Bob(alice.pubkey)\n",
    "    bob.set_weights(encrypted_weights, encrypted_intercept)\n",
    "    with timer() as t:\n",
    "        encrypted_scores = bob.encrypted_evaluate(X_test)\n",
    "\n",
    "    print(\"Alice: Decrypting Bob's scores\")\n",
    "    with timer() as t:\n",
    "        scores = alice.decrypt_scores(encrypted_scores)\n",
    "    error = np.mean(np.sign(scores) != y_test)\n",
    "    print(\"Error {:.3f} -- this is not known to Alice, who does not possess the ground truth labels\".format(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
