{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python PaillierEncryption\n",
    "The core cryptographic functionality in this project was provided by the PHE(Python PaillierEncryption) library, which is a Python implementation of the Paillier cryptosystem. The library is open-source and can be found on GitHub.\n",
    "\n",
    "@misc{PythonPaillier, author = {CSIRO's Data61}, title = {Python Paillier Library}, year = {2013}, publisher = {GitHub}, journal = {GitHub Repository}, howpublished = {\\url{https://github.com/data61/python-paillier}}, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/data61/python-paillier.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgkWIKXTWZ2t",
    "outputId": "0a0fe365-3ce6-4e7d-9f28-dc2d657cf9a6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import phe as paillier\n",
    "from contextlib import contextmanager\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgkWIKXTWZ2t",
    "outputId": "0a0fe365-3ce6-4e7d-9f28-dc2d657cf9a6"
   },
   "outputs": [],
   "source": [
    "def load_data_from_jsonl(file_path):\n",
    "    '''\n",
    "    Read a JSONL (JSON Lines) file and extracts email texts and their corresponding binary labels.\n",
    "    Labels are binary: spam (1) and ham/non-spam (0).\n",
    "    Parameters : file_path (str) - Path to the JSONL file.\n",
    "    Returns:\n",
    "        np.array - Array of email texts.\n",
    "        np.array - Array of binary labels (1 for spam, 0 for ham).\n",
    "    '''\n",
    "    texts, labels = [], []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            texts.append(entry['text'])  # Adapt based on the JSON schema\n",
    "            labels.append(1 if entry['label'] == 1 else 0)  # Keep label binary: spam (1), ham (0)\n",
    "    return np.array(texts), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unencrypted model\n",
    "This model is mainly for comparison with the encrypted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgkWIKXTWZ2t",
    "outputId": "0a0fe365-3ce6-4e7d-9f28-dc2d657cf9a6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "def process_datasets(train_file, test_file):\n",
    "    '''\n",
    "    Load, preprocess, and transform training and testing datasets.\n",
    "    This function vectorizes email texts using TF-IDF.\n",
    "    Parameters:\n",
    "        train_file (str) - Path to the training data file.\n",
    "        test_file (str) - Path to the testing data file.\n",
    "    Returns: Transformed training data (X_train_reduced), training labels (train_labels), test data (X_test_reduced) and test labels (test_labels).\n",
    "    '''\n",
    "\n",
    "    print(\"Reading training dataset.\")\n",
    "    train_texts, train_labels = load_data_from_jsonl(train_file)\n",
    "\n",
    "    print(\"Reading testing dataset.\")\n",
    "    test_texts, test_labels = load_data_from_jsonl(test_file)\n",
    "\n",
    "    # Text vectorization (Bag-of-Words with Tfidf)\n",
    "    vectorizer = TfidfVectorizer(use_idf=False, norm=None, stop_words='english', min_df=0.001)\n",
    "\n",
    "    # Fit vectorizer on training data and apply to test data\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "    # Ensure both classes are present in training set\n",
    "    if len(np.unique(train_labels)) < 2:\n",
    "        raise ValueError(\"Training data must have both spam and non-spam labels.\")\n",
    "\n",
    "    print(f\"Proportion of spam: {np.mean(train_labels == 1):.2f}, ham: {np.mean(train_labels == 0):.2f}\")\n",
    "\n",
    "    return X_train, train_labels, X_test, test_labels\n",
    "\n",
    "@contextmanager\n",
    "def measure_time():\n",
    "    # A context manager for measuring the execution time of a block of code.\n",
    "    start = time.perf_counter()\n",
    "    yield\n",
    "    end = time.perf_counter()\n",
    "    elapsed = end - start\n",
    "    print(f'Elapsed time: {elapsed:.2f} seconds')\n",
    "\n",
    "class SystemAdmin:\n",
    "    \"\"\"\n",
    "    SystemAdmin is responsible for training models (SVM, Logistic Regression, and Perceptron) and making prediction using test data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initializes SVM, Logistic Regression, and Perceptron models.\n",
    "        self.svm_model = SVC(kernel='linear')\n",
    "        self.lr_model = LogisticRegression()\n",
    "        self.perceptron_model = Perceptron()\n",
    "\n",
    "    def train_models(self, X, y):\n",
    "        #Trains the SVM, Logistic Regression, and Perceptron models using the provided training data.\n",
    "        self.svm_model.fit(X, y)\n",
    "        self.lr_model.fit(X, y)\n",
    "        self.perceptron_model.fit(X, y)\n",
    "\n",
    "    def make_prediction(self, model, X):\n",
    "        #Makes predictions using the given model and input data.\n",
    "        return model.predict(X)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_path = 'train.jsonl'\n",
    "    test_path = 'test.jsonl'\n",
    "\n",
    "    X_train, y_train, X_test, y_test = process_datasets(train_path, test_path)\n",
    "\n",
    "    # Admin operations\n",
    "    admin = SystemAdmin()\n",
    "\n",
    "    # Train models and record training time\n",
    "    training_times = {}\n",
    "    print(\"Training models.\")\n",
    "    for model_name, model in zip([\"SVM\", \"Logistic Regression\", \"Perceptron\"],\n",
    "                                 [admin.svm_model, admin.lr_model, admin.perceptron_model]):\n",
    "        start_time = time.perf_counter()\n",
    "        admin.train_models(X_train, y_train)\n",
    "        training_times[model_name] = time.perf_counter() - start_time\n",
    "\n",
    "    # Evaluate models and store results\n",
    "    results = []\n",
    "    for model_name, model in zip([\"SVM\", \"Logistic Regression\", \"Perceptron\"],\n",
    "                                 [admin.svm_model, admin.lr_model, admin.perceptron_model]):\n",
    "        # Unencrypted evaluation\n",
    "        start_time = time.perf_counter()\n",
    "        preds = admin.make_prediction(model, X_test)\n",
    "        test_time = time.perf_counter() - start_time\n",
    "\n",
    "        error_rate = np.mean(preds != y_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        results.append((model_name, \"Unencrypted\", error_rate, training_times[model_name], test_time, f1))\n",
    "\n",
    "    # Display results in a DataFrame\n",
    "    df_results = pd.DataFrame(results, columns=[\"Model\", \"Type\", \"Test Error\", \"Training Time\", \"Test Time\", \"F1 Score\"])\n",
    "    print(\"\\nResults Comparison:\")\n",
    "    print(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted Model\n",
    "Here we combine encryption and machine learning to create a privacy preserving spam detection model. Then we compare the results with those we got from unencrypted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_datasets(train_file, test_file, n_components=100):\n",
    "    \"\"\"\n",
    "    Load, preprocess, and transform training and testing datasets.\n",
    "    This function vectorizes email texts using TF-IDF and  applies Principal Component Analysis (PCA) to reduce the dimensionality of the feature space.\n",
    "    Parameters:\n",
    "        train_file : Path to the training data file.\n",
    "        test_file : Path to the testing data file.\n",
    "        n_components : Number of PCA components to reduce the feature space to (we have taken 100).\n",
    "    Returns: Transformed training data (X_train_reduced), training labels (train_labels), test data (X_test_reduced), and test labels (test_labels).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Reading training dataset.\")\n",
    "    train_texts, train_labels = load_data_from_jsonl(train_file)\n",
    "\n",
    "    print(\"Reading testing dataset.\")\n",
    "    test_texts, test_labels = load_data_from_jsonl(test_file)\n",
    "\n",
    "    # Text vectorization (Bag-of-Words with Tfidf)\n",
    "    vectorizer = TfidfVectorizer(use_idf=False, norm=None, stop_words='english', min_df=0.001)\n",
    "\n",
    "    # Fit vectorizer on training data and apply to test data\n",
    "    X_train = vectorizer.fit_transform(train_texts).toarray()  # Convert to dense array for PCA\n",
    "    X_test = vectorizer.transform(test_texts).toarray()\n",
    "\n",
    "    print(f'Original feature space size: {X_train.shape[1]}')\n",
    "\n",
    "    # Scale the data before applying PCA\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_reduced = pca.fit_transform(X_train_scaled)\n",
    "    X_test_reduced = pca.transform(X_test_scaled)\n",
    "\n",
    "    print(f'Reduced feature space size: {X_train_reduced.shape[1]}')\n",
    "\n",
    "    # Ensure both classes are present in training set\n",
    "    if len(np.unique(train_labels)) < 2:\n",
    "        raise ValueError(\"Training data must have both spam and non-spam labels.\")\n",
    "\n",
    "    print(f\"Proportion of spam: {np.mean(train_labels == 1):.2f}, ham: {np.mean(train_labels == 0):.2f}\")\n",
    "\n",
    "    return X_train_reduced, train_labels, X_test_reduced, test_labels\n",
    "\n",
    "@contextmanager\n",
    "def measure_time():\n",
    "    # A context manager for measuring the execution time of a block of code.\n",
    "    start = time.perf_counter()\n",
    "    yield\n",
    "    end = time.perf_counter()\n",
    "    elapsed = end - start\n",
    "    print(f'Elapsed time: {elapsed:.2f} seconds')\n",
    "\n",
    "class SystemAdmin:\n",
    "    \"\"\"\n",
    "    SystemAdmin is responsible for training models (SVM, Logistic Regression, and Perceptron), encrypting model parameters using Paillier encryption, and generating encryption keys.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initializing SVM, Logistic Regression, and Perceptron models.\n",
    "        self.svm_model = SVC(kernel='linear')\n",
    "        self.lr_model = LogisticRegression(max_iter=2000)  # Increased max_iter to 2000 for better convergence\n",
    "        self.perceptron_model = Perceptron()\n",
    "\n",
    "    def generate_paillier_keys(self, key_length):\n",
    "        \"\"\"\n",
    "        Generating Paillier public and private key pairs.\n",
    "        Parameters: key_length (int) - The length of the encryption key.\n",
    "        \"\"\"\n",
    "        self.pub_key, self.priv_key = paillier.generate_paillier_keypair(n_length=key_length)\n",
    "\n",
    "    def train_models(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the SVM, Logistic Regression, and Perceptron models using the provided training data.\n",
    "        Parameters:\n",
    "            X - Training data (features).\n",
    "            y - Training labels.\n",
    "        \"\"\"\n",
    "        self.svm_model.fit(X, y)\n",
    "        self.lr_model.fit(X, y)\n",
    "        self.perceptron_model.fit(X, y)\n",
    "\n",
    "    def make_prediction(self, model, X):\n",
    "        \"\"\"\n",
    "        Makes predictions using the given model and input data.\n",
    "        Parameters:\n",
    "            model (sklearn model) - Trained model (SVM, Logistic Regression, or Perceptron).\n",
    "            X - Input data (features).\n",
    "        Returns: Model predictions.\n",
    "        \"\"\"\n",
    "        return model.predict(X)\n",
    "\n",
    "    def encrypt_model_params(self, model):\n",
    "        \"\"\"\n",
    "        Encrypts the model parameters (weights and intercept).\n",
    "        Parameters: model - Trained model (SVM, Logistic Regression, or Perceptron).\n",
    "        Returns: Encrypted weights and intercept of the model.\n",
    "        \"\"\"\n",
    "\n",
    "        coefficients = model.coef_.toarray()[0, :] if hasattr(model.coef_, \"toarray\") else model.coef_[0, :]\n",
    "        encrypted_weights = [self.pub_key.encrypt(float(w)) for w in coefficients]\n",
    "        encrypted_intercept = self.pub_key.encrypt(float(model.intercept_[0]))\n",
    "        return encrypted_weights, encrypted_intercept\n",
    "\n",
    "    def decrypt_values(self, encrypted_values):\n",
    "        \"\"\"\n",
    "        Decrypts encrypted values.\n",
    "        Parameters: encrypted_values - List of encrypted values.\n",
    "        Returns: List of decrypted values.\n",
    "        \"\"\"\n",
    "        return [self.priv_key.decrypt(val) for val in encrypted_values]\n",
    "\n",
    "class ModelUser:\n",
    "    \"\"\"\n",
    "    ModelUser evaluates the encrypted models on test data. It initializes with the encrypted model parameters (weights and intercept) and calculates scores using encrypted feature vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, pub_key):\n",
    "        \"\"\"\n",
    "        Initializes the ModelUser with the public key for encryption.\n",
    "        Parameters: pub_key (PaillierPublicKey) - The public key for encryption.\n",
    "        \"\"\"\n",
    "        self.pub_key = pub_key\n",
    "\n",
    "    def initialize_model(self, enc_weights, enc_intercept):\n",
    "        \"\"\"\n",
    "        Initializes the model with encrypted weights and intercept.\n",
    "        Parameters:\n",
    "            enc_weights - Encrypted weights of the model.\n",
    "            enc_intercept - Encrypted intercept of the model.\n",
    "        \"\"\"\n",
    "        self.weights = enc_weights\n",
    "        self.intercept = enc_intercept\n",
    "\n",
    "    def calculate_encrypted_score(self, feature_vec):\n",
    "        \"\"\"\n",
    "        Calculates the encrypted score for a given feature vector.\n",
    "        Parameters: feature_vec - Input feature vector.\n",
    "        Returns: The encrypted score.\n",
    "        \"\"\"\n",
    "        score = self.intercept\n",
    "        # Loop through all features since it's now a dense vector\n",
    "        for idx in range(len(self.weights)):\n",
    "            score += feature_vec[idx] * self.weights[idx]\n",
    "        return score\n",
    "\n",
    "    def evaluate_model(self, X):\n",
    "        \"\"\"\n",
    "        Evaluates the encrypted model on the test dataset.\n",
    "        Parameters: X - Test data (features).\n",
    "        Returns: List of encrypted scores for each test sample.\n",
    "        \"\"\"\n",
    "        return [self.calculate_encrypted_score(X[i, :]) for i in range(X.shape[0])]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_path = 'train.jsonl'\n",
    "    test_path = 'test.jsonl'\n",
    "\n",
    "    # Process datasets and apply PCA for dimensionality reduction\n",
    "    X_train, y_train, X_test, y_test = process_datasets(train_path, test_path, n_components=100)\n",
    "\n",
    "    # Admin operations\n",
    "    admin = SystemAdmin()\n",
    "    admin.generate_paillier_keys(key_length=1024)\n",
    "\n",
    "    # Train models and record training time\n",
    "    training_times = {}\n",
    "    print(\"Training models.\")\n",
    "    for model_name, model in zip([\"SVM\", \"Logistic Regression\", \"Perceptron\"],\n",
    "                                 [admin.svm_model, admin.lr_model, admin.perceptron_model]):\n",
    "        start_time = time.perf_counter()\n",
    "        admin.train_models(X_train, y_train)\n",
    "        training_times[model_name] = time.perf_counter() - start_time\n",
    "\n",
    "    # Evaluate models (encrypted) and store results to the existing result[] list\n",
    "    for model_name, model in zip([\"SVM\", \"Logistic Regression\", \"Perceptron\"],\n",
    "                                 [admin.svm_model, admin.lr_model, admin.perceptron_model]):\n",
    "        # Encrypt model parameters\n",
    "        enc_weights, enc_intercept = admin.encrypt_model_params(model)\n",
    "\n",
    "        # User-side encrypted evaluation\n",
    "        user = ModelUser(admin.pub_key)\n",
    "        user.initialize_model(enc_weights, enc_intercept)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        enc_scores = user.evaluate_model(X_test)\n",
    "        encrypted_test_time = time.perf_counter() - start_time\n",
    "\n",
    "        decrypted_scores = admin.decrypt_values(enc_scores)\n",
    "        # Convert decrypted_scores to binary predictions (0 or 1)\n",
    "        binary_predictions = (np.sign(decrypted_scores) > 0).astype(int)\n",
    "        # Calculate F1 score\n",
    "        enc_f1 = f1_score(y_test, binary_predictions, average='binary')\n",
    "        # Calculate test error\n",
    "        enc_error_rate = np.mean(binary_predictions != y_test)\n",
    "        results.append((model_name, \"Encrypted\", enc_error_rate, training_times[model_name], encrypted_test_time, enc_f1))\n",
    "\n",
    "    # Display results in a DataFrame\n",
    "    df_results = pd.DataFrame(results, columns=[\"Model\", \"Type\", \"Test Error\", \"Training Time\", \"Test Time\", \"F1 Score\"])\n",
    "    print(\"\\nResults Comparison:\")\n",
    "    print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
