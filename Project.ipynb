{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/data61/python-paillier.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgkWIKXTWZ2t",
    "outputId": "0a0fe365-3ce6-4e7d-9f28-dc2d657cf9a6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load the dataset\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "file_path = 'test.jsonl'\n",
    "df = read_jsonl(file_path)\n",
    "\n",
    "# Text preprocessing\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):  # Convert to lowercase, Remove digits and special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Using Word2Vec for word embedding\n",
    "sentences = [text.split() for text in df['cleaned_text']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Aggregate word embeddings for each email (average of word embeddings)\n",
    "def get_sentence_embedding(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "df['embedding'] = df['cleaned_text'].apply(lambda x: get_sentence_embedding(x, word2vec_model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for unencrypted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train dataset for now\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR for unencrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression Model\n",
    "log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paillier Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "!pip install phe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os.path\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import phe as paillier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_data_from_jsonl(file_path):\n",
    "    \"\"\"Load data from a JSONL file.\"\"\"\n",
    "    emails = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            emails.append(entry['text'])  # Adjust based on your JSON field\n",
    "            labels.append(1 if entry['label_text'] == 'spam' else -1)  # Adjust label logic as necessary\n",
    "    return np.array(emails), np.array(labels)\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Get the Enron e-mails from disk.\n",
    "    Represent them as bag-of-words.\n",
    "    Shuffle and split train/test.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Importing dataset from disk...\")\n",
    "    # Load emails from your JSONL file\n",
    "    emails, labels = load_data_from_jsonl(file_path)\n",
    "\n",
    "    # Words count, keep only frequent words\n",
    "    count_vect = CountVectorizer(decode_error='replace', stop_words='english',\n",
    "                                  min_df=0.001)\n",
    "    X = count_vect.fit_transform(emails)\n",
    "\n",
    "    print('Vocabulary size: %d' % X.shape[1])\n",
    "\n",
    "    # Shuffle\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    X, y = X[perm, :], labels[perm]\n",
    "\n",
    "    # Split train and test\n",
    "    split = 500  # Number of samples to use for training\n",
    "    X_train, X_test = X[-split:, :], X[:-split, :]\n",
    "    y_train, y_test = y[-split:], y[:-split]\n",
    "\n",
    "    # Check unique labels in training set\n",
    "    unique_labels = np.unique(y_train)\n",
    "    print(\"Unique labels in training set:\", unique_labels)\n",
    "    print(y_train)\n",
    "\n",
    "    # Ensure both classes are present\n",
    "    if len(unique_labels) < 2:\n",
    "        raise ValueError(\"Training set must contain both spam and ham examples.\")\n",
    "\n",
    "    print(\"Labels in trainset are {:.2f} spam : {:.2f} ham\".format(\n",
    "        np.mean(y_train == 1), np.mean(y_train == -1)))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer():\n",
    "    \"\"\"Helper for measuring runtime\"\"\"\n",
    "    time0 = time.perf_counter()\n",
    "    yield\n",
    "    print('[elapsed time: %.2f s]' % (time.perf_counter() - time0))\n",
    "\n",
    "\n",
    "class Alice:\n",
    "    \"\"\"Represents the model owner who trains the SVM model.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = SVC(kernel='linear')  # SVM with linear kernel\n",
    "\n",
    "    def generate_paillier_keypair(self, n_length):\n",
    "        self.pubkey, self.privkey = paillier.generate_paillier_keypair(n_length=n_length)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def encrypt_weights(self):\n",
    "        # Convert sparse matrix to dense format (NumPy array)\n",
    "        coef = self.model.coef_.toarray()[0, :]\n",
    "        # Encrypt the coefficients\n",
    "        encrypted_weights = [self.pubkey.encrypt(float(coef[i])) for i in range(coef.shape[0])]\n",
    "        # Encrypt the intercept\n",
    "        encrypted_intercept = self.pubkey.encrypt(float(self.model.intercept_[0]))\n",
    "        return encrypted_weights, encrypted_intercept\n",
    "\n",
    "    def decrypt_scores(self, encrypted_scores):\n",
    "        return [self.privkey.decrypt(s) for s in encrypted_scores]\n",
    "\n",
    "\n",
    "\n",
    "class Bob:\n",
    "    \"\"\"Scores local plaintext data using the encrypted model.\"\"\"\n",
    "    def __init__(self, pubkey):\n",
    "        self.pubkey = pubkey\n",
    "\n",
    "    def set_weights(self, weights, intercept):\n",
    "        self.weights = weights\n",
    "        self.intercept = intercept\n",
    "\n",
    "    def encrypted_score(self, x):\n",
    "        score = self.intercept\n",
    "        _, idx = x.nonzero()\n",
    "        for i in idx:\n",
    "            score += x[0, i] * self.weights[i]\n",
    "        return score\n",
    "\n",
    "    def encrypted_evaluate(self, X):\n",
    "        return [self.encrypted_score(X[i, :]) for i in range(X.shape[0])]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = 'test.jsonl'  # Specify your JSONL file path here\n",
    "    X, y, X_test, y_test = preprocess_data(file_path)\n",
    "\n",
    "    print(\"Alice: Generating paillier keypair\")\n",
    "    alice = Alice()\n",
    "    alice.generate_paillier_keypair(n_length=1024)\n",
    "\n",
    "    print(\"Alice: Learning spam classifier\")\n",
    "    with timer() as t:\n",
    "        alice.fit(X, y)\n",
    "\n",
    "    print(\"Classify with model in the clear -- what Alice would get having Bob's data locally\")\n",
    "    with timer() as t:\n",
    "        error = np.mean(alice.predict(X_test) != y_test)\n",
    "    print(\"Error {:.3f}\".format(error))\n",
    "\n",
    "    print(\"Alice: Encrypting classifier\")\n",
    "    with timer() as t:\n",
    "        encrypted_weights, encrypted_intercept = alice.encrypt_weights()\n",
    "\n",
    "    print(\"Bob: Scoring with encrypted classifier\")\n",
    "    bob = Bob(alice.pubkey)\n",
    "    bob.set_weights(encrypted_weights, encrypted_intercept)\n",
    "    with timer() as t:\n",
    "        encrypted_scores = bob.encrypted_evaluate(X_test)\n",
    "\n",
    "    print(\"Alice: Decrypting Bob's scores\")\n",
    "    with timer() as t:\n",
    "        scores = alice.decrypt_scores(encrypted_scores)\n",
    "    error = np.mean(np.sign(scores) != y_test)\n",
    "    print(\"Error {:.3f} -- this is not known to Alice, who does not possess the ground truth labels\".format(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
