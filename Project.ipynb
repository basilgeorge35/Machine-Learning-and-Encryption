{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/data61/python-paillier.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgkWIKXTWZ2t",
    "outputId": "0a0fe365-3ce6-4e7d-9f28-dc2d657cf9a6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "np.random.seed(99)\n",
    "\n",
    "def load_data_from_jsonl(file_path):\n",
    "    #Reads JSONL file and extracts email texts and their corresponding labels.\n",
    "    texts, labels = [], []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            texts.append(entry['text'])  # Adapt based on the JSON schema\n",
    "            labels.append(1 if entry['label_text'] == 'spam' else -1)  # Convert labels to numerical\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "def process_datasets(train_file, test_file):\n",
    "    #Load, preprocess, and transform training and testing datasets.\n",
    "    print(\"Reading training dataset.\")\n",
    "    train_texts, train_labels = load_data_from_jsonl(train_file)\n",
    "\n",
    "    print(\"Reading testing dataset.\")\n",
    "    test_texts, test_labels = load_data_from_jsonl(test_file)\n",
    "\n",
    "    # Text vectorization (Bag-of-Words with Tfidf, without IDF)\n",
    "    vectorizer = TfidfVectorizer(use_idf=False, norm=None, stop_words='english', min_df=0.001)\n",
    "\n",
    "    # Fit vectorizer on training data and apply to test data\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "    print(f'Feature space size: {X_train.shape[1]}')\n",
    "\n",
    "    # Ensure both classes are present in training set\n",
    "    if len(np.unique(train_labels)) < 2:\n",
    "        raise ValueError(\"Training data must have both spam and non-spam labels.\")\n",
    "\n",
    "    print(f\"Proportion of spam: {np.mean(train_labels == 1):.2f}, ham: {np.mean(train_labels == -1):.2f}\")\n",
    "\n",
    "    return X_train, train_labels, X_test, test_labels\n",
    "\n",
    "@contextmanager\n",
    "def measure_time():\n",
    "    #Context manager to measure execution time.\n",
    "    start = time.perf_counter()\n",
    "    yield\n",
    "    end = time.perf_counter()\n",
    "    elapsed = end - start\n",
    "    print(f'Elapsed time: {elapsed:.2f} seconds')\n",
    "\n",
    "class SystemAdmin:\n",
    "    # Responsible for model training\n",
    "    def __init__(self):\n",
    "        self.svm_model = SVC(kernel='linear')\n",
    "        self.lr_model = LogisticRegression()\n",
    "        self.perceptron_model = Perceptron()\n",
    "\n",
    "    def train_models(self, X, y):\n",
    "        #Train SVM, Logistic Regression, and Perceptron models.\n",
    "        self.svm_model.fit(X, y)\n",
    "        self.lr_model.fit(X, y)\n",
    "        self.perceptron_model.fit(X, y)\n",
    "\n",
    "    def make_prediction(self, model, X):\n",
    "        #Make predictions with the specified model.\n",
    "        return model.predict(X)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_path = 'train.jsonl'\n",
    "    test_path = 'test.jsonl'\n",
    "\n",
    "    X_train, y_train, X_test, y_test = process_datasets(train_path, test_path)\n",
    "\n",
    "    # Admin operations\n",
    "    admin = SystemAdmin()\n",
    "\n",
    "    # Train models and record training time\n",
    "    training_times = {}\n",
    "    print(\"Training models.\")\n",
    "    for model_name, model in zip([\"SVM\", \"Logistic Regression\", \"Perceptron\"],\n",
    "                                 [admin.svm_model, admin.lr_model, admin.perceptron_model]):\n",
    "        start_time = time.perf_counter()\n",
    "        admin.train_models(X_train, y_train)\n",
    "        training_times[model_name] = time.perf_counter() - start_time\n",
    "\n",
    "    # Evaluate models and store results\n",
    "    results = []\n",
    "    for model_name, model in zip([\"SVM\", \"Logistic Regression\", \"Perceptron\"],\n",
    "                                 [admin.svm_model, admin.lr_model, admin.perceptron_model]):\n",
    "        # Unencrypted evaluation\n",
    "        start_time = time.perf_counter()\n",
    "        preds = admin.make_prediction(model, X_test)\n",
    "        test_time = time.perf_counter() - start_time\n",
    "\n",
    "        error_rate = np.mean(preds != y_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        results.append((model_name, \"Unencrypted\", error_rate, training_times[model_name], test_time, f1))\n",
    "\n",
    "    # Display results in a DataFrame\n",
    "    df_results = pd.DataFrame(results, columns=[\"Model\", \"Type\", \"Test Error\", \"Training Time\", \"Test Time\", \"F1 Score\"])\n",
    "    print(\"\\nResults Comparison:\")\n",
    "    print(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for unencrypted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using train dataset for now\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Training\n",
    "# svm_model = SVC(kernel='linear', random_state=42)\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# y_pred = svm_model.predict(X_test)\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR for unencrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train Logistic Regression Model\n",
    "# log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "# log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Detailed classification report\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paillier Encryption with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "np.random.seed(99)\n",
    "\n",
    "def load_data_from_jsonl(file_path):\n",
    "    # Reads JSONL file and extracts email texts and their corresponding labels.\n",
    "    texts, labels = [], []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            texts.append(entry['text'])  # Adapt based on the JSON schema\n",
    "            labels.append(1 if entry['label_text'] == 'spam' else -1)  # Convert labels to numerical\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "def process_datasets(train_file, test_file, n_components=100):\n",
    "    # Load, preprocess, and transform training and testing datasets, with PCA for dimensionality reduction.\n",
    "    print(\"Reading training dataset.\")\n",
    "    train_texts, train_labels = load_data_from_jsonl(train_file)\n",
    "\n",
    "    print(\"Reading testing dataset.\")\n",
    "    test_texts, test_labels = load_data_from_jsonl(test_file)\n",
    "\n",
    "    # Text vectorization (Bag-of-Words with Tfidf, without IDF)\n",
    "    vectorizer = TfidfVectorizer(use_idf=False, norm=None, stop_words='english', min_df=0.001)\n",
    "\n",
    "    # Fit vectorizer on training data and apply to test data\n",
    "    X_train = vectorizer.fit_transform(train_texts).toarray()  # Convert to dense array for PCA\n",
    "    X_test = vectorizer.transform(test_texts).toarray()\n",
    "\n",
    "    print(f'Original feature space size: {X_train.shape[1]}')\n",
    "\n",
    "    # Scale the data before applying PCA\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_reduced = pca.fit_transform(X_train_scaled)\n",
    "    X_test_reduced = pca.transform(X_test_scaled)\n",
    "\n",
    "    print(f'Reduced feature space size: {X_train_reduced.shape[1]}')\n",
    "\n",
    "    # Ensure both classes are present in training set\n",
    "    if len(np.unique(train_labels)) < 2:\n",
    "        raise ValueError(\"Training data must have both spam and non-spam labels.\")\n",
    "\n",
    "    print(f\"Proportion of spam: {np.mean(train_labels == 1):.2f}, ham: {np.mean(train_labels == -1):.2f}\")\n",
    "\n",
    "    return X_train_reduced, train_labels, X_test_reduced, test_labels\n",
    "\n",
    "@contextmanager\n",
    "def measure_time():\n",
    "    # Context manager to measure execution time.\n",
    "    start = time.perf_counter()\n",
    "    yield\n",
    "    end = time.perf_counter()\n",
    "    elapsed = end - start\n",
    "    print(f'Elapsed time: {elapsed:.2f} seconds')\n",
    "\n",
    "class SystemAdmin:\n",
    "    # Responsible for model training, encryption, and key generation.\n",
    "    def __init__(self):\n",
    "        self.svm_model = SVC(kernel='linear')\n",
    "        self.lr_model = LogisticRegression(max_iter=2000)  # Increased max_iter to 2000 for better convergence\n",
    "        self.perceptron_model = Perceptron()\n",
    "\n",
    "    def generate_paillier_keys(self, key_length):\n",
    "        self.pub_key, self.priv_key = paillier.generate_paillier_keypair(n_length=key_length)\n",
    "\n",
    "    def train_models(self, X, y):\n",
    "        # Train SVM, Logistic Regression, and Perceptron models.\n",
    "        self.svm_model.fit(X, y)\n",
    "        self.lr_model.fit(X, y)\n",
    "        self.perceptron_model.fit(X, y)\n",
    "\n",
    "    def make_prediction(self, model, X):\n",
    "        # Make predictions with the specified model.\n",
    "        return model.predict(X)\n",
    "\n",
    "    def encrypt_model_params(self, model):\n",
    "        # Encrypt model parameters.\n",
    "        coefficients = model.coef_.toarray()[0, :] if hasattr(model.coef_, \"toarray\") else model.coef_[0, :]\n",
    "        encrypted_weights = [self.pub_key.encrypt(float(w)) for w in coefficients]\n",
    "        encrypted_intercept = self.pub_key.encrypt(float(model.intercept_[0]))\n",
    "        return encrypted_weights, encrypted_intercept\n",
    "\n",
    "    def decrypt_values(self, encrypted_values):\n",
    "        # Decrypt encrypted values.\n",
    "        return [self.priv_key.decrypt(val) for val in encrypted_values]\n",
    "\n",
    "class ModelUser:\n",
    "    # User who evaluates data using encrypted models.\n",
    "    def __init__(self, pub_key):\n",
    "        self.pub_key = pub_key\n",
    "\n",
    "    def initialize_model(self, enc_weights, enc_intercept):\n",
    "        self.weights = enc_weights\n",
    "        self.intercept = enc_intercept\n",
    "\n",
    "    def calculate_encrypted_score(self, feature_vec):\n",
    "        score = self.intercept\n",
    "        # Loop through all features since it's now a dense vector\n",
    "        for idx in range(len(self.weights)):\n",
    "            score += feature_vec[idx] * self.weights[idx]\n",
    "        return score\n",
    "\n",
    "    def evaluate_model(self, X):\n",
    "        # Score test data using the encrypted model.\n",
    "        return [self.calculate_encrypted_score(X[i, :]) for i in range(X.shape[0])]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_path = 'train.jsonl'\n",
    "    test_path = 'test.jsonl'\n",
    "\n",
    "    # Process datasets and apply PCA for dimensionality reduction\n",
    "    X_train, y_train, X_test, y_test = process_datasets(train_path, test_path, n_components=100)\n",
    "\n",
    "    # Admin operations\n",
    "    admin = SystemAdmin()\n",
    "    admin.generate_paillier_keys(key_length=1024)\n",
    "\n",
    "    # Train models and record training time\n",
    "    training_times = {}\n",
    "    print(\"Training models.\")\n",
    "    for model_name, model in zip([\"SVM\", \"Logistic Regression\", \"Perceptron\"],\n",
    "                                 [admin.svm_model, admin.lr_model, admin.perceptron_model]):\n",
    "        start_time = time.perf_counter()\n",
    "        admin.train_models(X_train, y_train)\n",
    "        training_times[model_name] = time.perf_counter() - start_time\n",
    "\n",
    "    # Evaluate models (encrypted) and store results\n",
    "    results = []\n",
    "    for model_name, model in zip([\"SVM\", \"Logistic Regression\", \"Perceptron\"],\n",
    "                                 [admin.svm_model, admin.lr_model, admin.perceptron_model]):\n",
    "        # Encrypt model parameters\n",
    "        enc_weights, enc_intercept = admin.encrypt_model_params(model)\n",
    "\n",
    "        # User-side encrypted evaluation\n",
    "        user = ModelUser(admin.pub_key)\n",
    "        user.initialize_model(enc_weights, enc_intercept)\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        enc_scores = user.evaluate_model(X_test)\n",
    "        encrypted_test_time = time.perf_counter() - start_time\n",
    "\n",
    "        decrypted_scores = admin.decrypt_values(enc_scores)\n",
    "        enc_error_rate = np.mean(np.sign(decrypted_scores) != y_test)\n",
    "        enc_f1 = f1_score(y_test, np.sign(decrypted_scores))\n",
    "        results.append((model_name, \"Encrypted\", enc_error_rate, training_times[model_name], encrypted_test_time, enc_f1))\n",
    "\n",
    "    # Display results in a DataFrame\n",
    "    df_results = pd.DataFrame(results, columns=[\"Model\", \"Type\", \"Test Error\", \"Training Time\", \"Test Time\", \"F1 Score\"])\n",
    "    print(\"\\nResults Comparison:\")\n",
    "    print(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
